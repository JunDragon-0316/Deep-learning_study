1. 알고리즘의 학습과 미분
- 손실함수를 최소화하는 매개변수 찾기
- 경사 하강법을 위해 미분 필요 -> 손실 함수를 모든 매개변수로 미분.
- 학습환경이 주어졌을 때, 손실 함수를 매개변수로 여러 번 미분해야 한다.

2. 의존성이 있는 함수의 계산
- 동일 연산이 2회 필요하므로 , 중복되는 계산이 1회 발생한다.

3. 동적 계획법(Dynamic Programming)
- 첫 계산 시 값을 저장하므로 중복 계산이 발생하지 않는다.

4. 연쇄 법칙(Chain Rule)
- 연속된 두 함수의 미분은, 각 함수의 미분을 연쇄적으로 곱한것과 같다.

5. 심층 신경망의 미분 
- 출력계층의 미분 : 연쇄 법칙을 이용하려면 손실 함수의 미분이 필요하다.
- 은닉계층의 미분 : 연쇄 법칙을 이용하려면 손실 함수, 출력계층의 미분이 필요하다
                    출력 계층, 손실 함수의 미분을 저장해 두면(동적 계획법) 중복 연산을 피할 수 있다.

6. 순방향 추론(Forward Inference)
- 현재 매개변수에서 손실 값을 계산하기 위해 순차적인 연산을 수행 하는 것을, 순방향 추론이라고 한다.
- 학습을 마친 후 알고리즘을 사용할 떄에는 순방향 추론을 사용한다.

7. 역전파 학습법(Back-Propagation)
- 심층 신경망의 미분을 계산하기 위해, 연쇄 법칙과 동적 계획법을 이용하여 효율적으로 계산할 수 있다.
- 이 과정이 순방향 추론과 반대로 이루어지기 때문에 이를 역전파 학습법이라 한다.
