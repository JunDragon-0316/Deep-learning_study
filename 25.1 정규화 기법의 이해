1. Loss VS. Complexity
- Loss에 집중 : 학습 데이터에 대한 시니뢰도가 높음 . 학습 데이터에 속하지 않은 입력에 취약
- Complexity를 낮춤 : 모델의 복잡도가 지나치게 높아지지 않도록 제약. 데이터 학습보다 일반화에 투자
- Cost = Loss(Data|Model) + λComplexity(Model)

2. L-2 정규화 L-2 Regularization
- L-2 Regularization (Ridge)는 가중치의 L-2 Norm을 최소화 하는 방법이다.
- 가중치가 정규분포의 형태를 이루도록한다.
- Complexity(Model) = 1/N ∑ 2/1 W2 = 1/2 ||w||2
                          i      i          
- 아주 큰 가중치에 대해 패널티 부여
- 더 구불구불한 것 보다, 더 평평한 것을 선호
- 베이지언 사전 확률 분포 (정규 분포)
- 람다 값이 크면 가중치는 정규 분포에 가깝게 나타난다.
- 람다 값이 0에 가까울 수록 정규화가 이루어지지 않으며, 가중치는 평평한 분포를 지향한다.

3. 사전 확률 분포 Prior Probability Distribution
- 사전 확률 분포는 a priori라고도 하며, 데이터를 보기 전에 확률 분포를 예측하는 것을 말한다.
- 베이지언 통계학에서 많이 사용하는 방법이다.

4. L-1 정규화 L-1 Regularization
- L-1 Regularization (Lasso)는 가중치의 L-1 Norm을 최소화 하는 방법이다.
- 가중치가 라플라스 분포의 형태를 이루도록한다.
- Complexity(Model) = 1/N ∑ |W2| = ||w||1
                          i       
- 가중치의 절대값에 패널티를 줌
- 값이 양수 또는 음수로 존재하며 줄이려함
- 값이 희소(Sparse)해 지는 특성이 있음
- 베이지언 사전 확률 분포 (라플라스 분포)

5. Lasso Vs. Ridge
구분  Lasso                           Ridge
수식  ||W||1                          1/2||W||2
미분  1 w<0                             w
     -1 w>0
특성 -가중치 값을 정확하게 0으로 만듦  - 큰 가중치의 값을 작게 만듦
     - 중요한 특징을 선택하는 효과     - 모델 전반적인 복잡도를 감소시키는 효과
     - 모델에 Sparsity를 가함          - 가중치의 값이 0이 되게 하지는 못함

6. MSE Loss
- 보통 별 의심 없이 써오던 MSE Loss는 평균을 나타내는 특성이 있다.
- 에러가 클수록 더욱 큰 패널티
- 데이터의 평균- 존재하지 않는값
- 데이터를 Smoothing하는 효과
- Outline에 취약한 단점이 있다.
- argmin ∑||yi - Wxi||2
      w               2

7. MAE Loss
- MAe Loss는 중간값의 특성이 있으며, Outline에 강건한 특성이 있다.
- 에러가 커져도 동일한 패널티
- 데이터의 중간값- 존재하는 정확한 값
- 존재하는 값을 사용하여 샤프한 특성
- 적게 존재하는 값을 무시하는 특성
- argmin ∑|yi - Wxi|
      w               

