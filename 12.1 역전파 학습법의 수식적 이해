1. 합성 함수와 연쇄 법칙

- 연쇄 법칙 (Chaine Rule)
- 직렬 연결된 두 함수의 미분
- x -> f(x) -> y -> g(y) -> z
- 미분과 연쇄 법칙 : 연쇄 법칙을 이용한 미분의 계산.
- 합성 함수의 미분 : 겉미분과 속미분
- 연쇄 법칙의 확장 -> 연속된 함수의 미분을 각각 미분의 곱으로 표현할 수 있다.

2. 역전파 학습법 (Back Prapagation)
- 합성 함수로서의 심층 신경망 : 
- 심층 신경망의 각 Layer를 하나의 함수로 본다면, 신경망을 합성 함수로 표현할 수 있다.
- 학습 관점에서 본 심층 신경망 :
- 이미 손실을 구했다면, 데이터셋의 입력과 출력은 학습과정에서 중요하지 않다.
- 손실을 최소화하는 파라미터만 찾으면 되기 때문이다.
- 심층신경망의 연쇄 법칙 :
- 미분하고자 하는 경로 사이에 있는 모든 미분값을 곱하면, 원하는 미분을 구할수 있다.

3. 전결합 계층의 미분
- 출력 : 입력, 가중치, 편향을통해 미분을 할수 있어야한다.
- Activation function의 미분을 알고 있다면, 쉽게 Fully connected lalyer를 미분할 수 있다.

4. sigmoid 함수의 미분
- 초창기 신경망에 가장 만힝 쓰인 Sigmoid 활성 함수의 미분. 

5. 역전파 알고리즘 
- 정방향(Feed forward) -> Loss -> 역방향(Backward)
- 학습 데이터로 정방향 연산을 하여 Loss를 구한다.
- 정방향 연산시, 계층별로 BP에 필요한 중간 결과를 저장한다.
- Loss를 각 파라미터로 미분한다, 연쇄법칙(역방향 연산)을 이용한다.
- 마지막 계층부터 하나씩 이전 계층으로 연쇄적으로 계산한다.
- 역방향 연산시, 정방향 연산에서 저장한 중간 결과를 사용한다.
- 미분의 연쇄 법칙과 각 함수의 수식적 미분을 이용하면, 단 한번의 손실함수 평가로 미분을 구할수 있다.
- 단 , 중간 결과를 저장해야 하므로 메모리를 추가로 사용한다.
