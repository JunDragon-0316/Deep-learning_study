1. 얕은 신경망을 깊게 이해하기
- 얕은 신경망에 대한 깊은 이해가 깊은 신경망에 대한 이해를 도울 것이다.

2. 뉴런의 수학적 표현
- 뉴런은 수학적으로 두 벡터의 내적으로 쉽게 표현할수 있다.
- y = a   N-1
       (   ∑   WiXi + b) <-편향(Bias)
          i=0
         t =(트렌스포지) 
- y = a(WtX+b) 

- 편향은 좌표를 원점에서 벗어나게 해주는 특성

3. 전결합 계층의 수학적 표현
- FC 계층은 여러 개의 뉴런을 한 곳에 모아둔 것으로, Matrix 곱셉 연산으로 표현된다.

- yo = a(WtoX+bo) ,  y1 = a(Wt1X+b1) ....  ym-1 = a(Wtm-1X+bm-1) =  y = a(WX+b)
- W= [Wo,W1,...Wm-1]t , b= [bo,b1...bm-1]t 

4. 입력계층 (Input Layer)
- 아무런 연산도 일어나지 않음
- 신경망의 입력을 받아 다음계층으로 넘기는 역할
- 무엇을 입력으로 주어 야하는가 ? 특징 추출문제 
- 계층의 크기 = Node의 개수 = 입력 Scalar의 수 = 입력 Vactor의 길이
- x = [x0, x1 ....xn-1]t
열vactor       scalar

5. 은닉계층 (Hidden Layer)
- 은닉계층은 입력 계층과 연결된 전결합 계층이다.
- 입출력 관점에서 볼 떄 드러나지 않는다고 하여, 은닉 계층이라 한다.
- 복잡한 문제를 해결 할 수 있게 하는 핵심적인 계층.
- 얕은 신경망에서 1개의 은닉 계층만을 사용한다. 
- h = an(Wnx + bh)   h=(hidden)

6. 출력 계층 (Output Layer)
- 출력 계층은 은닉 계층의 다음으로 오는 전결합 계층이다
- 신경망의 외부로 출력신호를 전달하는데 사용된다.
- 신경망의 기능은 출력 계층의 활성 함수에 의해 결정된다.
- 출력 계층의 크기 = 출력 scalar 수 = 출력 벡터의 길이
- y = ao(Woh + bo) 
