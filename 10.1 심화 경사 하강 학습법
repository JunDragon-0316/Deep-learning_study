1. 미분과 기울기 (Gradient)
- 기울기는 스칼라를 벡터로 미분한 것이며, 벡터의 각 요소로 미분하면 된다.

2. 경사하강법
- f(x)의 값이 변하지 않을 때 까지, 스텝을 반복한다.

3. 학습률의 선택
- 한 스텝의 크기는 기울기의 크기에도 비례하므로, 학습률이 극단적으로 크면 값이 발산한다.
- inf , Non 이 나올경우 학습률을 줄여줘야 한다.

4. 비볼록 함수
- 우리가 마주칠 대부분의 문제는 비볼록 함수이므로, 단순한 경사 하강법으로는 한계가 있다.

5. 지역 최솟값 
- 경사 하강법을 사용할 경우, 초기값에 따라 Local Minimum 에 빠질 위험이 있다.

6. 안장점 Saddle Point
- 기울기가 0이 되지만 극값이 아닌 지점을 말한다. 경사 하강법은 안장점에서 벗어나지 못한다.

7. 관성 (Momentum)
- 돌이 굴러 떨어지듯, 이동 벡터를 이용해 이전 기울기에 영향을 받도록 하는 방법
- Local minimum, 과 잡음을 대처할수 있다.
- 이동 벡터를 추가로 사용하므로, 경사 하강법 대비 2배의 메모리를 사용한다.

8. 적응적 기울기 (AdaGrad)
- Adaptive Gradient : 변수별로 학습율이 달라지게 조절하는 알고리즘
- 기울기가 커서 학습이 많이 된 변수는 학습율을 감소 시켜, 다른 변수들이 잘 학습되도록 한다.
- gt가 계속해서 커져서 학습이 오래 진행되면 더이상 학습이 이루어지지 않는 단점이 있다.

9. RMSprop
- AdaGrad의 문제점을 개선한 방법으로, 합 대신 지수평균을 사용
- 변수 간의 상대적인 학습율 차이는 유지하면서 gt가 무한정 커지지 않아 학습을 오래 할 수 있다.

10. Adam
- Adaptive moment estimation(Adam) : RMSProp 과 Momentum의 장점을 결합한 알고리즘
- 가장 최신의 기술(state-of-the-art) 이며, 딥러닝에 가장 많이 사용된다.
