1.일반 경사 하강법 (Vanilla Gradient Descent)
- 합습데이터(N개의 학습샘플) -> 뉴럴 데이터 -> 손실함수 -> 역전파 알고리즘 -> Gradient Descent
- 일반 경사 하강법의 경우, Gradient를 한번 업데이트 하기 위해 모든 학습 데이터를 사용한다.

2. 확률적 경사 하강법(Stochastic Gradient Descent)
- 합습데이터(N개의 학습샘플) -> 뉴럴 데이터 -> 손실함수 -> 역전파 알고리즘 -> 
  Stochastic Gradient Descent (Batch Size개의 Gradient 평균)
- Stochastic GD(SGD)의 경우, Gradient를 한번 업데이트 하기 위해 일부의 데이터만을 사용한다.

3. 미니 배치 학습법
- 학습 데이터 전체를 한번 학습하는 것을 Epoch, 한 번 Gradient를 구하는 단위를 Batch라고 한다.
- Internal Covariate Shift : 학습 과정에서 계층별로 입력의 데이터 분포가 달라지는 현상

4. 배치 정규화 (Batch Normalization)
- 학습 과정에서 각 배치별로 평균과 분산을 이용해 정규화하는 계층을 배치 정규화 계층이라 한다.
- 학습 단계 (Training Phase): 정규화로 인해, 모든 계층의 Feature가 동일한 Scale이 되어 학습률 결정에 유리하다. 
                             추가적인 Scale, Bias를 학습하여 Activation에 적합한 분포로 변환할 수 있다. 
- 추론 단계 (Inference Phase): 추론 과정에서는 평균과 분산을 이동 평균 (또는 지수 평균)하여 고정
                               정규화와 추가 Scale,Bias를 결합하여 단일 곱, 더하기 연산으로 줄일수 있음.
