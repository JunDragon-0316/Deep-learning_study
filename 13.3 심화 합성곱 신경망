1. GoolLeNet(Inception)
- 22 Layers // 에러율 6.7% , VGG 19 Layer // 에러율 7.4%
- 네트워크를 더욱 더 깊게 만들고자 하는 노력에서 나왔다.
- Inception 모듈: 다양한 크기의 합성곱 계층을 한번에 계산한다.
- 연산량을 줄이기 위한 1x1 합성곱 계층. 이러한 구조를 Bottlenenk 구조라고 부른다
- Bottleneck 구조의 활용으로 , Receptive field를 유지하면서 파라미터 수와 연산량을 줄였다.
- 추가 분류기 사용: 역전파에서 기울기 소실이 발생하는 것을 방지하기 위해, 같은 문제를 여러 단계에서 풀도록 하였다.

2. Residual Network(ResNet)
- Facebook에서 활약중인 Kaiming He의 명품 딥러닝 네트워크
- 기존 20계층 수준의 네트워크를 152계층까지 늘이는 성과를 거두었다.
- Skip connection이 주요한 역할을 한다.
- Feature를 추출하기 전 후를 더하는 특징이 있다. (Residual 구조)
- 일반적인 구조에서 표현 가능한 것은, Residual 구조에서도 표현 가능하다.
- Identity Mapping: 한 단위의 특징 맵을 추출하고 난 후에 활성 함수를 적용하는 것이 상식이었다.
                    하지만 개선된 구조에서는 Pre-Activation을 제안했다
- Pre-Activation : Conv-BN-ReLU 구조를 BN-ReLU-Conv 구조로 변경한 것으로 성능이 개선되었다.
                   후자의 경우 Gradient Highway가 형성되어 극적인 효과를 얻는다.

3. Densely Connectec ConvNets (DenseNet)
- ResNet의 아이디어를 계승하여 좋은 성능을 이끌어 냄
- Densblock 을 제안하고 있고, 내에는 ResNet과 같이 Pre Activation 구조(BN-ReLU-Conv)구조를 사용한다.
- Input -> Conv -> Dense Block -> conv -> Poding ... -> Linear -> Prediction
- Densblock : 이리저리 연결된 Skip Connection이 조금 복잡해 보일수 있지만, 다른 관점에서 보면
              이전 특징 맵에 누적해서 Concatenate하는 결과와 같다.
- Bottleneck 구조 : 레이어가 깊어지면서 연산량이 급격히 증가하는 것을 막기 위해,
                    1x1 Conv를 이용한 Bottleneck Layer를 사용하였다.
