1. 회귀 (Regression) 출력이 연속된 값으로 추정하는 값

2. 단순 선형 회귀 (Liner Regression) : 데이터를 가장 잘 표현하는 선형식을 찾는 동작 
- y =wx + b y(출력), x(입력)
- 독립변수(입력)이 하나이므로, 추정해야 할 변수도 하나, 단 편향을 포함하면 2개가 된다.
- W = arg min 1 {   (y - yi)² = 평균 제곱 에러 (Mean Squared Error ;MSE)
           w  N   ∑    2           MSE를 최소로 하는 W를 찾는것

3.평균 제곱 오차 MSE
- x(고양이) -> H(x;θ) 길이 , 체중 추정 알고리즘 - > 
- y = [10.5 ,3.6]....
- MSE 를 이용해 고양이의 길이와 체중의 오차를 종합적으로 판단할수 있다.

4. 다중 선형 회귀
- y = Wx + b -> y = WoXo + W1X1 .... Wn-1Xn-1 + b
- n-1  WiXi + b
   ∑
  i=0 
- WtX + b
- 단일 입력이 아닌, 다중 입력을 받을 경우에는 변수가 확장되어 벡터의 내적이 된다.

5. 다중 선형 회귀의 기하하적 해석
- Y = Wx + b -> y = Wx + b -> y = WoXo + W1X1 .... Wn-1Xn-1 + b -> WtX + b (HyperPlane)
- 변수가 하나 추가될 때마다 차원이 하나씩 추가 된다. 직선-> 평면 -> 초평면

6. 얕은 신경망과 회귀 알고리즘 
- 입력, 은닉계층 = h = ah (WhX + bh)
- 은닉, 출력계층 y = WTh + bo (선형회귀)
                      o
- 얕은 신경망으로 회귀를 수행할 경우, 출력 계층은 선형 회귀와 동일하다.
- 입력 계층에서 은닉계층으로 추가적인 변환이 있다는 것이 다른점이다.

7. 은닉 계층과 회귀
- 선형적으로 분포되지 않는 입력 -> 선형적으로 분포 하지않는 은닉계층(특징)
  -> 선형회귀 입력 Space를 기준으로 보면 회귀 곡선이 된다.
