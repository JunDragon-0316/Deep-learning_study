1. Query, Key-Value, 
- Query : 질의. 찾고자 하는 대상 
- Key : 키. 저장된 데이터를 찾고자 할 때 참조하는 값
- Value : 값. 저장되는 데이터
- Dictionary : Key - Value Pair로 이루어진 집합
## ex) 어벤져스 
- 2019 : End Game
- 2018 : Infinity War
- 2012 : Avengers
- Querying : 질의-응답 -> 비교 -> 값 출력

2. Attention mechanism
- Q에 대해 어떤 K가 유사한지 비교하고, 유사도를 반영하여, V들을 합성한것이 Attention Value이다

3. Seq2seq
- Seq2seq - Key - Value : 대부분의 Attention network에서는 key와 value를 같은 값을 사용한다.
                         seq2seq에서는 Encoder의 hidden layer들을 key 와 value로 사용한다.
- Seq2seq - Query : seq2seq에서는 Encoder의 hidden layer들을 Query로 사용한다.
                    주의할 점은, Encoder와 달리 하나앞선 time- step의 hidden layer를 사용한다는 점.
- Seq2seq - Attention mechanism : i번째 decoder에 대해서 ai의 attention value를 얻는다.
- Seq2seq - Application : RNN으로 Hidden state를 입력하기 전에, attention value를 concatenate하여 입력한다.

4. Attention is all you need
- 번역 문제에 RNN과 CNN을 사용하지 않고, Attention만을 이용하여 State-of-the-art 성능을 이끌어낸 연구

5. 네트워크 특성
- seq2seq와 유사한 Transformer 구조 사용
- 제안하는 Scaled Dot-product Attention과 , 이를 병렬로 나열한 Multi-Head Attention 블록이 알고리즘 핵심
- RNN의 BPTT와 같은 과정이 없으므로 병렬 계산 가능
- 입력된 단어의 위치를 표현하기 위해 Positional Encoding 사용

6. Transformer Vs. Seq2seq
- Input 입력단어의 가짓수 , 입력 sequence 길이 n
- Output 출력단어의 가짓수 , 입력 sequence 길이 m -> one- hot Encoding
- Word Embedding -> one- hot Encoding 된 단어를 실수 형태로 변경하면서 차원의 수를 줄이는 방법
- Positional Encoding : 시간적 위치별로 고유의 code를 생성하여 더하는 방식
                        전체 Sequence의 길이 중 상대적 위치에 따라 고유의 벡터를 생성하여 Embedding된 벡터에 더해줌
- Scaled Dot - product Attention : Query , Key-value의 구조를 띄움
                                  Q와 K의 비교함수는 Dot-product와 scale로 이루어짐
                                  Mask를 이용해 Illegal connection의 attention을 금지
                                  softmax로 유사도를 0~1의 값으로 Normalize
                                  유사도와 v를 결합해 Attention Value 계산
- Multi - Head Attention : Linear 연산 (Matrix mult)를 이용해 Q,K,V의 차원을 감소
                           Q와 K의 차원이 다른경우 이를 이용해 동일하게 맞춤
                           h개의 Attention Layer를 병렬적으로 사용- 더 넓은 계층 
                           출력 직전 Linear 연산을 이용해 Attention Value의 차원을 필요에 따라 변경
                           이 메커니즘을 통해 병렬 계산에 유리한 구조를 가지게 됨
- Masked Multi-Head Attention : Self- Attention 에서 자기 자신을 포함한 미래의 Attention을 구하지 않기 떄문에 ,
                                masking을 사용한다.
                                self attention = key query value 가 그대로 들어감 , feature 추출
- Position- wise Feed Forward : 단어 하나마다, 입력으로 들어가서 출력으로 나온다.
- Add & Norm : Layer Normalization(배치에 영향을 받지않음) 을 사용 
- Output Softmax : Linear 연산을 이용해서 출력 단어 종류의 수에 맞춤
                   softmax를 이용해 어떤 단어인지 classification문제 해결
- Attention is really : Seq2seq와 유사한 Transformer구조 사용
                        제안하는 scaled Dot-Product Attention과 이를 병렬로 나열한 Multi Head Attention 블록이 알고리즘 핵심
                        RNN의 BPTT와 같은 과정이 없으므로 병렬 계산 가능
                        입력된 단어의 위치를 표현하기 위해 Positional Encoding 사용 
